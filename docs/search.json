[
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Traffic Summary of each of WI’s 72 County Profiles\n\n\n\nMarkdown\n\n\nData Viz\n\n\n\nCrash data summaries for each of the 72 counties. The reports include dynamic maps showing crash locations and hotspots.\n\n\n\nMay 1, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nWorking with API and JSON format\n\n\n\nAPI\n\n\nData analysis\n\n\n\nThe task was to find the average sentence length of OWI (operating while intoxicated) offenders on their 3rd or more OWI citation using DOJ court case data.\n\n\n\nOct 4, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nWisDOT Crash Comparison Dashboard\n\n\n\nShiny\n\n\nData Viz\n\n\nAPI\n\n\n\nThis is a Shiny dashboard that compares crashes by quarter within a selected County or Municipality. This analysis was previously done manually in Excel, needing to find the…\n\n\n\nOct 1, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nWisconsin Traffic Crash Facts\n\n\n\nQuarto\n\n\n\nOriginally a 100+ page PDF book, I made it into an online crash facts book using Quarto. The book provides tables and charts of aggregated crash data. Past versions were…\n\n\n\nAug 1, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nAnalysis of OWI ratios\n\n\n\nData analysis\n\n\n\nThis research project involved calculating risk ratios of OWI (operating while intoxicated) offenders from across multiple datasets.\n\n\n\nNov 1, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nCoffee Roasting Profiler\n\n\n\nShiny\n\n\nData Viz\n\n\n\nAn ameateur coffee roaster, I made this dashboard to visualize my coffee roasts.\n\n\n\nAug 4, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nWisDOT Crash Statistics Dashboard\n\n\n\nShiny\n\n\nData Viz\n\n\n\nWisDOT uses Community Maps, a website that displays a map of crashes in near real-time. I wanted the user…\n\n\n\nJan 4, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nR Package: wisdotcrashdatabase\n\n\n\nPackage development\n\n\n\nI developed this package to make data import and data analysis much easier inside an R environment.\n\n\n\nAug 20, 2020\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/owi.html",
    "href": "projects/owi.html",
    "title": "Analysis of OWI ratios",
    "section": "",
    "text": "What I learned\nThis project involved the compilation of 5 data sets to calculate OWI (operating while intoxicated) ratios broken down by age group, race and sex. In order to combine these datasets, I had to rename columns and recode variables so datasets can be joined. The entire analysis I wrote functions to allow for any combo of age, sex, and race (the variables the study involved).\n\n\nHighlighted packages\ndata.table, dplyr"
  },
  {
    "objectID": "projects/crash_package.html",
    "href": "projects/crash_package.html",
    "title": "R Package: wisdotcrashdatabase",
    "section": "",
    "text": "What I learned\nThe typical work flow was using SAS to get the data and then doing the analysis in Excel. Doing everything in R would be more efficient. It came to a point where I was constantly sourcing the same scripts so it was logical to turn these scripts into an R package. The package has functions that query certain crash flags so I don’t have to look up the actual query. On the backend, data is stored in a SQL database, one that I created and updates automatically on a weekly basis. Importing data multiple times a day and unable to store tihs data in the cloud, I found duckdb to be the fastest solution (as compared to csv, SQLite, and fst).\n\n\nHighlighted packages\nThe dashboard uses data.table, duckdb, fst, and purrr."
  },
  {
    "objectID": "projects/crash_dashboard.html",
    "href": "projects/crash_dashboard.html",
    "title": "WisDOT Crash Statistics Dashboard",
    "section": "",
    "text": "What I learned\nThrough performance testing via shinyloadtest, I found an enormous bottleneck through simply loading the data. As a solution, I created a SQLite database as the database is saved locally inside the package. While this dramitically improved performance, data was still being mapped on-the-fly. Ideally the data should be stored in a spatial database. This app also uses modules so server and ui are much cleaner while the code is easier to debug.\n\n\nHighlighted packages\nThe dashboard uses plotly, leaflet, leaflet.extras2 and leafgl under a golem framework."
  },
  {
    "objectID": "projects/county_profiles.html",
    "href": "projects/county_profiles.html",
    "title": "Traffic Summary of each of WI’s 72 County Profiles",
    "section": "",
    "text": "Dane County’s Traffic Safety Summary\n\nWhat I learned\nI learned to create a parameterized R Markdown report. I used our crash data API for the maps using the googleway package. This project was previously done using Excel and Publisher and took many months. This project fully automated the report by using “County” as the parameter.\n\n\nHighlighted packages\nplotly, googleway, and kableExtra"
  },
  {
    "objectID": "noncode_projects/lost_city.html",
    "href": "noncode_projects/lost_city.html",
    "title": "Madison’s ‘Lost City’ - A Look Into the Failed 1920’s Lake Forest Development",
    "section": "",
    "text": "This article was originally published on Reddit in 2016 and was even featured in The Capital Times.\n\nA map geek, a planning enthusiast and a sucker for the unknown, I decided to delve a little deeper into one of Madison’s greatest mysteries, the “Lost City.” You may be thinking, “Is it even real?” or “Why the heck are there paved paths in the Arboretum?” After scoping out online resources, a visit to the Wisconsin Historical Society, and an in-situ visit to the area, I want to share what I found.\nLet’s start with some context of planning. The industrial revolution brought an influx of people to flock to urban areas. By the late 1910s, it was the first time in human history that the country’s urban population outnumbered the rural population. This brought on the perception that cities were rifled with poor sanitation, terrible pollution, and overcrowding. Areas that were once used as public space or parks were now houses, businesses, and industries. People wanted to escape dirty, crowded cities, but did not want to live in farm country.\n\nStarting in the 1890s and lasting til the 1920s, a new wave of planning cities emerged deemed the City Beautiful Movement. Led by developers, planners, and architects, the idea was that classic beauty and monumental grandeur of the city would inspire feelings of civic loyalty and moral rectitude. By incorporating civic centers, parks, and grand boulevards, city beautification would reduce crime, reduce density, and increase green space. While Madison was already established as being a pristine place to live, there were worries with what the city’s predicted continued growth will lead.\n\nDeveloper and president of the Lake Forest Company, Chandler B. Chapman attempted to hop on the bandwagon in taking one step further in beautifying Madison by building a residential paradise called Lake Forest. An 800-acre epic development with over 1,000 lots, Lake Forest would be an escape from city life with numerous parks, its own public transit, and a city center. The company promoted Lake Forest by calling it “by far the best planned city residential section in Wisconsin” and “the most beautiful, modern, healthful, and desirable dwelling spot in the Northeast.”\n\nThis southeastern section of the development was started with many of the roads built along with a few houses.\n Note: Block A is the Civic Center and Block B is St. Cyr Circle while Capitol Avenue intersects these two circles.  \nThe center of life for Lake Forest residents, the Civic Center was planned to have small shops and businesses lined around the circle. Separating land uses, such as commercial, industrial, and residential, was a new phenomenon in planning. The only building still standing today is the pumping station which, I believe, is still in use.\n\nA network of lagoons were proposed for two prominent reasons: to provide waterfront access to residents and to stabilize the marshy grounds.\n\nThis was one grandeur development! To entice people in purchasing property, the company heavily advertised. Above is a 1920 advertisement placed in the Wisconsin State Journal. Additionally, a bimonthly newsletter called the “Lake Forester” that ran for 1.5 years from 1921-1922 was written to chronicle the progress of the development.\n\nIn 1921, the Lake Forest Company broke grounds: roads were pored with concrete, dredging was being done and housing foundations were laid. This is Capital Avenue under construction. Planned to be 120 feet wide with sidewalks on each side with an unforgettable view of the Capital Building, the size would have been comparable to Washington Avenue today. Street cars were planned to run in the center, providing transportation to the downtown. You can see St. Mary’s Hospital to the slight left.\n\nWhile at least five houses (some sources say 7 houses) were completed, none of them were ever occupied. EDIT: 10/8/2016 An Arboretum researcher says some of the houses are actually still standing and occupied.\n\nWhat happened next, nobody foresaw. A combination of problems led to the collapse and eventual decay of the development. Dredging proved insufficient: roads and foundations were sinking due to marshy soils. In 1920, only 61 lots were sold while the company’s financial planner went bankrupt. The Great Depression and World War II didn’t help either. Strapped for cash, the Lake Forest Company sold some of the land to the University of Wisconsin for arboretum purposes in the 1930s. Ultimately the development failed. This area today is the Lost City Forest in the Arboretum, perhaps a different kind of paradise.\n\nYup, there’s actually Indian effigy mounds here!  This 1941 plat map shows two effigy mounds located north of Lake Forest. Furthermore, a hand-drawn 1915 map of effigy mounds I found on Arboretum’s online map shows a group of four mounds here, one looking like a panther. Today this is now a residential area around Marshall Parkway. It’s private property and I wonder if the mounds still exist,\nThe mound site at St. Cyr Circle the article mentions, I found no records of a panther mound here. The 1915 map shows two long narrow mounds at this location while another map indicates that no mounds exist. However, there is a large group of at least 8 mounds located inside Wingra Woods, just west of Lake Forest.\n\nA 1937 aerial photo of the area shows the remains of this epic development. As you can seen, only some of the roads were completed. Today the roads east of the former Civic Center is completely developed with houses and apartments, while the other roads remain crumbling into history.\n\nToday, not much remains. A walk into Lost City Forest you find mysteriously paved paths nearly a century old. If you look at the Imagery basemap at ArcGIS.com you could actually see outlines of these roads.\n My findings cannot be complete without a stroll through the woods!\n\n\nI bet some of you are tempted to look for the remains. The Arboretum hosts a once-a-year Lost City tour, typically at the end of October. I went to the presentation and I highly recommend it.\nThe Arboretum advises hikers to not wonder off the trails as to protect the environment. I don’t want to encourage nor discourage people from doing so, just wanted to share a piece of this fascinating mystery.\nIf you’re curious, a few adventurers sought to find the Lost City. Check ’em out here:\n\nhttps://badgerherald.com/artsetc/2015/09/14/madisons-lost-city-inside-the-forgotten-remains-of-lake-forest/\nhttp://never365.blogspot.com/2013/11/day-177-finding-lost-city.html\nhttps://www.youtube.com/watch?v=GpYXDLR4_f0&feature=youtu.be\n\nHope you enjoyed this!\nSources:\n\nArcGIS Online\nLake Forester, 1920 - 1921 found at University of Wisconsin Collection\nMadison:the illustrated sesquicentennial history Vol. 1 1856 - 1931\nWisconsin Historical Aerial Image Finder\nWisconsin Historical Society"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Jacci Ziebert",
    "section": "",
    "text": "I am a data analyst and R programmer. My passion for R sparked when I felt frustrated of the mundane workflow at my previous job from SAS/Excel and knew there had to be a better way. I took an online programming class and immediately applied what I learned to my job.\nSince then, I’ve built an R package to improve work flow, developed Shiny web apps, and automated long reports. I enjoy translating data into a concise and meaningful visual for a general audience in either a dashboard or a report.\nWhen I’m not coding, I enjoy graphing my coffee roasts, playing strategy board games, and cycling.\n\n\n\nGetting ready for a morning gravel ride"
  },
  {
    "objectID": "index.html#greetings",
    "href": "index.html#greetings",
    "title": "Jacci Ziebert",
    "section": "",
    "text": "I am a data analyst and R programmer. My passion for R sparked when I felt frustrated of the mundane workflow at my previous job from SAS/Excel and knew there had to be a better way. I took an online programming class and immediately applied what I learned to my job.\nSince then, I’ve built an R package to improve work flow, developed Shiny web apps, and automated long reports. I enjoy translating data into a concise and meaningful visual for a general audience in either a dashboard or a report.\nWhen I’m not coding, I enjoy graphing my coffee roasts, playing strategy board games, and cycling.\n\n\n\nGetting ready for a morning gravel ride"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Jacci Ziebert",
    "section": "",
    "text": "I am a data analyst and R programmer. My passion for R sparked when I felt frustrated of the mundane workflow at my previous job from SAS/Excel and knew there had to be a better way. I took an online programming class and immediately applied what I learned to my job.\nSince then, I’ve built an R package to improve work flow, developed Shiny web apps, and automated long reports. I enjoy translating data into a concise and meaningful visual for a general audience in either a dashboard or a report.\nWhen I’m not coding, I enjoy graphing my coffee roasts, playing strategy board games, and cycling.\n\n\n\nGetting ready for a morning gravel ride"
  },
  {
    "objectID": "about.html#greetings",
    "href": "about.html#greetings",
    "title": "Jacci Ziebert",
    "section": "",
    "text": "I am a data analyst and R programmer. My passion for R sparked when I felt frustrated of the mundane workflow at my previous job from SAS/Excel and knew there had to be a better way. I took an online programming class and immediately applied what I learned to my job.\nSince then, I’ve built an R package to improve work flow, developed Shiny web apps, and automated long reports. I enjoy translating data into a concise and meaningful visual for a general audience in either a dashboard or a report.\nWhen I’m not coding, I enjoy graphing my coffee roasts, playing strategy board games, and cycling.\n\n\n\nGetting ready for a morning gravel ride"
  },
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "Curriculum Vitae",
    "section": "",
    "text": "Wisconsin Department of Health Services\n\n\nIS Business Automation Specialist June 2023 - Present\n\n\n\n\n\nWisconsin Department of Transportation\n\n\nHighway Safety Data Analyst May 2019 - June 2023\n\nDeveloped an R package that makes data pulls and data analysis much easier inside an R environment, slashing time spent fulfilling data requests into a manner of minutes\nWrote a parameterized R Markdown to fully automate the creation of 72 crash data county profiles\nUsed an API to collect court case data then flattening JSON file, cleaning data, and running analysis\nDeveloped a Shiny dashboard to compare quarterly crash trends\nImproved relationship between WisDOT and UW by establishing a set of uniform queries used for data pulls and crash analysis\n\n\n\n\n\n\nMandli Communications\n\n\nData Processor Dec 2016 - May 2019\n\nAnalyze, classify, and process LiDAR data maintaining above average speed, occasionally perform quality control\n\n\n\n\n\n\nWisconsin Department of Transportation\n\n\nTraffic Forecaster (Part-time) Feb 2014 – June 2018\n\nCompleted statewide traffic forecast requests using travel demand models and regression analysis of historical counts\nProvided expertise on completed forecast requests to MPOs, DOT regions and consultants\nConducted research and learned VBA to develop a macro-enabled Excel worksheet, improving and streamlining the department’s method to forecast turning movement counts\n\n\n\n\n\n\nWisconsin Department of Transportation\n\n\nBicycle and Pedestrian Intern (Part-time) Mar 2015 – July 2015\n\nLearned ADA standards in order to digitize and assess statewide curb ramps and sidewalks in ArcMap\nLearned about bike/pedestrian safety and design and the benefits of complete streets through five day-long courses"
  },
  {
    "objectID": "cv.html#professional-experience",
    "href": "cv.html#professional-experience",
    "title": "Curriculum Vitae",
    "section": "",
    "text": "Wisconsin Department of Health Services\n\n\nIS Business Automation Specialist June 2023 - Present\n\n\n\n\n\nWisconsin Department of Transportation\n\n\nHighway Safety Data Analyst May 2019 - June 2023\n\nDeveloped an R package that makes data pulls and data analysis much easier inside an R environment, slashing time spent fulfilling data requests into a manner of minutes\nWrote a parameterized R Markdown to fully automate the creation of 72 crash data county profiles\nUsed an API to collect court case data then flattening JSON file, cleaning data, and running analysis\nDeveloped a Shiny dashboard to compare quarterly crash trends\nImproved relationship between WisDOT and UW by establishing a set of uniform queries used for data pulls and crash analysis\n\n\n\n\n\n\nMandli Communications\n\n\nData Processor Dec 2016 - May 2019\n\nAnalyze, classify, and process LiDAR data maintaining above average speed, occasionally perform quality control\n\n\n\n\n\n\nWisconsin Department of Transportation\n\n\nTraffic Forecaster (Part-time) Feb 2014 – June 2018\n\nCompleted statewide traffic forecast requests using travel demand models and regression analysis of historical counts\nProvided expertise on completed forecast requests to MPOs, DOT regions and consultants\nConducted research and learned VBA to develop a macro-enabled Excel worksheet, improving and streamlining the department’s method to forecast turning movement counts\n\n\n\n\n\n\nWisconsin Department of Transportation\n\n\nBicycle and Pedestrian Intern (Part-time) Mar 2015 – July 2015\n\nLearned ADA standards in order to digitize and assess statewide curb ramps and sidewalks in ArcMap\nLearned about bike/pedestrian safety and design and the benefits of complete streets through five day-long courses"
  },
  {
    "objectID": "cv.html#awards",
    "href": "cv.html#awards",
    "title": "Curriculum Vitae",
    "section": " Awards",
    "text": "Awards\n\n2022 Data Science Olympian: Collaborated with a team to compete in a Kaggle Challenge with Women in Big Data (WiBD)\nWisDOT Crash Dashboard won runner-up under Interactive Map category: 2021 Wisconsin Land Information Association map contest. View the project."
  },
  {
    "objectID": "cv.html#education",
    "href": "cv.html#education",
    "title": "Curriculum Vitae",
    "section": " Education",
    "text": "Education\n\n\n\n\n\nUniversity of Wisconsin - Madison\n\n\nMaster of Science 2012 - 2014\n\nUrban and Regional Planning\nCertificate in Transportation Management and Policy\n\n\n\n\n\n\nUniversity of Wisconsin - Oshkosh\n\n\nBachelors of Science 2006 - 2010\n\nGeography (GIS) and Urban Planning"
  },
  {
    "objectID": "noncode_projects/bike_map.html",
    "href": "noncode_projects/bike_map.html",
    "title": "Subway-Style Bike Path Map of Madison, WI",
    "section": "",
    "text": "### What I learned\nI wanted to combine my interests of biking and maps, so I made this map of my hometown of Madison, WI. I started off with using QGIS to get the streets via a TIGER shapefile. Then in Illustrator, I traced over these roads. As I wanted to maintain the subway style, the rule was that all lines and polygons had to use 45 degree angles."
  },
  {
    "objectID": "noncode_projects.html",
    "href": "noncode_projects.html",
    "title": "Non-Coding Projects",
    "section": "",
    "text": "Subway-Style Bike Path Map of Madison, WI\n\n\n\nQGIS\n\n\nAdobe Illustrator\n\n\n\n\n\n\n\nNov 18, 2018\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMadison’s ‘Lost City’ - A Look Into the Failed 1920’s Lake Forest Development\n\n\n\nWriter\n\n\n\n\n\n\n\nJul 24, 2016\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/crash_comparison.html",
    "href": "projects/crash_comparison.html",
    "title": "WisDOT Crash Comparison Dashboard",
    "section": "",
    "text": "View the Dashboard\n\nWhat I learned\nKnowing this dashboard would be viewed on a variety of screen sizes, I used dynamic font sizes (i.e. 1.2em) as opposed to static font sizes (i.e. 12pt). I also wrote CSS as to make printing of the entire dashboard possible on two pages.\n\n\nHighlighted packages\nThe dashboard uses dashboardthemes, ggplot2 and ggtext using the golem framework."
  },
  {
    "objectID": "projects/crash_facts.html",
    "href": "projects/crash_facts.html",
    "title": "Wisconsin Traffic Crash Facts",
    "section": "",
    "text": "2021 Wisconsin Crash Facts\n\nWhat I learned\nThis project gave me an excuse to learn Quarto! I spent most time writing functions to standardize the table format, to render ggplot charts, and to aggregate the data. Then it was just a matter of creating each chapter using these functions.\n\n\nHighlighted packages\ngt, gtExtras, quarto"
  },
  {
    "objectID": "projects/json_api.html",
    "href": "projects/json_api.html",
    "title": "Working with API and JSON format",
    "section": "",
    "text": "What I learned\nThis was done it two parts 1) Writing functions that pulls data from our court case API into a JSON format over a certain time period and for certain citations. This data was flattened and compiled into a single dataframe and exported. And 2) Finding the sentence length for a certain citation for each case. This was tricky as one case may have multiple citations while sentence lengths can be found in multiple branches of the flattened JSON. I solved this issue by finding which branch a certain location was found and replaced part of the branch name with where the sentence length location. I was able to extract the sentence length with this ‘renaming.’ to calculate the average sentence length.\n\n\nHighlighted packages\njsonlite, httr"
  },
  {
    "objectID": "projects/roasting_dashboard.html",
    "href": "projects/roasting_dashboard.html",
    "title": "Coffee Roasting Profiler",
    "section": "",
    "text": "Dashboard  Code\n\nWhat I learned\nWhile I use Artisan, an open-source software used to record coffee roasts, I wanted an interactive chart to explore my roasts.\nThe tricky part was adding the RoR (rate of return) curves (the blue and purple ines) onto the graph as this is calculated via an algorithm written in Python inside Artisan. I found this piece of code and rewrote part of it so it would work for my app. I used the reticulate package so the app can read Python code.\n\n\nHighlighted packages\nThe dashboard uses reticulate, formattable, plotly, and DT under a golem framework."
  }
]