[
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "An Analysis of James Hoffman’s ‘The Great American Coffee Taste Test Live Stream’\n\n\n\nQuarto\n\n\nAuthor\n\n\nData analysis\n\n\nData Viz\n\n\n\nI used a survey with 4,000+ responses taken from participants in the live event to analysis coffee habit, preferences, and coffee tastes.\n\n\n\nDec 12, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nTraffic Summary of each of WI’s 72 County Profiles\n\n\n\nMarkdown\n\n\nData Viz\n\n\n\nCrash data summaries for each of the 72 counties. The reports include dynamic maps showing crash locations and hotspots.\n\n\n\nMay 1, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nWisDOT Crash Comparison Dashboard\n\n\n\nShiny\n\n\nData Viz\n\n\nAPI\n\n\n\nThis is a Shiny dashboard that compares crashes by quarter within a selected County or Municipality. This analysis was previously done manually in Excel, needing to find the…\n\n\n\nOct 1, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nWisconsin Traffic Crash Facts\n\n\n\nQuarto\n\n\n\nOriginally a 100+ page PDF book, I made it into an online crash facts book using Quarto. The book provides tables and charts of aggregated crash data. Past versions were…\n\n\n\nAug 1, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nAnalysis of OWI ratios\n\n\n\nData analysis\n\n\n\nThis research project involved calculating risk ratios of OWI (operating while intoxicated) offenders from across multiple datasets.\n\n\n\nNov 1, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nCoffee Roasting Profiler\n\n\n\nShiny\n\n\nData Viz\n\n\n\nAn ameateur coffee roaster, I made this dashboard to visualize my coffee roasts.\n\n\n\nAug 4, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nWorking with API and JSON format\n\n\n\nAPI\n\n\nData analysis\n\n\n\nThe task was to find the average sentence length of OWI (operating while intoxicated) offenders on their 3rd or more OWI citation using DOJ court case data.\n\n\n\nFeb 1, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nWisDOT Crash Statistics Dashboard\n\n\n\nShiny\n\n\nData Viz\n\n\n\nWisDOT uses Community Maps, a website that displays a map of crashes in near real-time. I wanted the user…\n\n\n\nJan 4, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nR Package: wisdotcrashdatabase\n\n\n\nPackage development\n\n\n\nI developed this package to make data import and data analysis much easier inside an R environment.\n\n\n\nAug 20, 2020\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/owi.html",
    "href": "projects/owi.html",
    "title": "Analysis of OWI ratios",
    "section": "",
    "text": "What I learned\nThis project involved the compilation of 5 data sets to calculate OWI (operating while intoxicated) ratios broken down by age group, race and sex. In order to combine these datasets, I had to rename columns and recode variables so datasets can be joined. The entire analysis I wrote functions to allow for any combo of age, sex, and race (the variables the study involved).\n\n\nHighlighted packages\ndata.table, dplyr"
  },
  {
    "objectID": "projects/crash_package.html",
    "href": "projects/crash_package.html",
    "title": "R Package: wisdotcrashdatabase",
    "section": "",
    "text": "What I learned\nThe typical work flow was using SAS to get the data and then doing the analysis in Excel. Doing everything in R would be more efficient. It came to a point where I was constantly sourcing the same scripts so it was logical to turn these scripts into an R package. The package has functions that query certain crash flags so I don’t have to look up the actual query. On the backend, data is stored in a SQL database, one that I created and updates automatically on a weekly basis. Importing data multiple times a day and unable to store tihs data in the cloud, I found duckdb to be the fastest solution (as compared to csv, SQLite, and fst).\n\n\nHighlighted packages\nThe dashboard uses data.table, duckdb, fst, and purrr."
  },
  {
    "objectID": "projects/crash_dashboard.html",
    "href": "projects/crash_dashboard.html",
    "title": "WisDOT Crash Statistics Dashboard",
    "section": "",
    "text": "What I learned\nThrough performance testing via shinyloadtest, I found an enormous bottleneck through simply loading the data. As a solution, I created a SQLite database as the database is saved locally inside the package. While this dramitically improved performance, data was still being mapped on-the-fly. Ideally the data should be stored in a spatial database. This app also uses modules so server and ui are much cleaner while the code is easier to debug.\n\n\nHighlighted packages\nThe dashboard uses plotly, leaflet, leaflet.extras2 and leafgl under a golem framework."
  },
  {
    "objectID": "projects/county_profiles.html",
    "href": "projects/county_profiles.html",
    "title": "Traffic Summary of each of WI’s 72 County Profiles",
    "section": "",
    "text": "Dane County’s Traffic Safety Summary\n\nWhat I learned\nI learned to create a parameterized R Markdown report. I used our crash data API for the maps using the googleway package. This project was previously done using Excel and Publisher and took many months. This project fully automated the report by using “County” as the parameter.\n\n\nHighlighted packages\nplotly, googleway, and kableExtra"
  },
  {
    "objectID": "noncode_projects.html",
    "href": "noncode_projects.html",
    "title": "Non-Coding Projects",
    "section": "",
    "text": "Subway-Style Bike Path Map of Madison, WI\n\n\n\nQGIS\n\n\nAdobe Illustrator\n\n\n\n\n\n\n\nNov 18, 2018\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMadison’s ‘Lost City’ - A Look Into the Failed 1920’s Lake Forest Development\n\n\n\nAuthor\n\n\n\n\n\n\n\nJul 24, 2016\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "noncode_projects/bike_map.html",
    "href": "noncode_projects/bike_map.html",
    "title": "Subway-Style Bike Path Map of Madison, WI",
    "section": "",
    "text": "What I learned\nI wanted to combine my interests of biking and maps, so I made this map of my hometown of Madison, WI. I started off with using QGIS to get the streets via a TIGER shapefile. Then in Illustrator, I traced over these roads. As I wanted to maintain the subway style, the rule was that all lines and polygons had to use 45 degree angles."
  },
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "Curriculum Vitae",
    "section": "",
    "text": "Government Resource Center at The Ohio State University\n\n\nShiny Web App Developer August 2024-Present\n\nDevelop Shiny web apps\n\n\n\n\n\n\nWisconsin Department of Health Services\n\n\nIS Business Automation Specialist June 2023 - August 2024\n\nAutomated DHS’s org charts for 7,000+ employees, saving 60 hours annually\nServe as liaison for remote work policy by determining which employees meet criteria for remote work through combining multiple unclean data sources\n\n\n\n\n\n\nWisconsin Department of Transportation\n\n\nHighway Safety Data Analyst May 2019 - June 2023\n\nDeveloped an R package that makes data pulls and data analysis much easier inside an R environment, slashing time spent fulfilling data requests into a matter of minutes\nWrote a parameterized R Markdown to fully automate the creation of 72 crash data county profiles\nWrote code to call an API to collect extensive court case data then flattening JSON file, cleaning data, and running analysis\nLearned Shiny to develop a Crash Dashboard with multi-selection capabilities, interactive charts, and a dynamic map\nDeveloped a Shiny dashboard to compare quarterly crash trends\nImproved relationship between WisDOT and UW by establishing a set of uniform queries used for data pulls and crash analysis\n\n\n\n\n\n\nMandli Communications\n\n\nData Processor Dec 2016 - May 2019\n\nAnalyze, classify, and process LiDAR data maintaining above average speed, occasionally perform quality control\n\n\n\n\n\n\nWisconsin Department of Transportation\n\n\nTraffic Forecaster (Part-time) Feb 2014 – June 2018\n\nCompleted statewide traffic forecast requests using travel demand models and regression analysis of historical counts\nProvided expertise on completed forecast requests to MPOs, DOT regions and consultants\nConducted research and learned VBA to develop a macro-enabled Excel worksheet, improving and streamlining the department’s method to forecast turning movement counts\n\n\n\n\n\n\nWisconsin Department of Transportation\n\n\nBicycle and Pedestrian Intern (Part-time) Mar 2015 – July 2015\n\nLearned ADA standards in order to digitize and assess statewide curb ramps and sidewalks in ArcMap\nLearned about bike/pedestrian safety and design and the benefits of complete streets through five day-long courses"
  },
  {
    "objectID": "cv.html#professional-experience",
    "href": "cv.html#professional-experience",
    "title": "Curriculum Vitae",
    "section": "",
    "text": "Government Resource Center at The Ohio State University\n\n\nShiny Web App Developer August 2024-Present\n\nDevelop Shiny web apps\n\n\n\n\n\n\nWisconsin Department of Health Services\n\n\nIS Business Automation Specialist June 2023 - August 2024\n\nAutomated DHS’s org charts for 7,000+ employees, saving 60 hours annually\nServe as liaison for remote work policy by determining which employees meet criteria for remote work through combining multiple unclean data sources\n\n\n\n\n\n\nWisconsin Department of Transportation\n\n\nHighway Safety Data Analyst May 2019 - June 2023\n\nDeveloped an R package that makes data pulls and data analysis much easier inside an R environment, slashing time spent fulfilling data requests into a matter of minutes\nWrote a parameterized R Markdown to fully automate the creation of 72 crash data county profiles\nWrote code to call an API to collect extensive court case data then flattening JSON file, cleaning data, and running analysis\nLearned Shiny to develop a Crash Dashboard with multi-selection capabilities, interactive charts, and a dynamic map\nDeveloped a Shiny dashboard to compare quarterly crash trends\nImproved relationship between WisDOT and UW by establishing a set of uniform queries used for data pulls and crash analysis\n\n\n\n\n\n\nMandli Communications\n\n\nData Processor Dec 2016 - May 2019\n\nAnalyze, classify, and process LiDAR data maintaining above average speed, occasionally perform quality control\n\n\n\n\n\n\nWisconsin Department of Transportation\n\n\nTraffic Forecaster (Part-time) Feb 2014 – June 2018\n\nCompleted statewide traffic forecast requests using travel demand models and regression analysis of historical counts\nProvided expertise on completed forecast requests to MPOs, DOT regions and consultants\nConducted research and learned VBA to develop a macro-enabled Excel worksheet, improving and streamlining the department’s method to forecast turning movement counts\n\n\n\n\n\n\nWisconsin Department of Transportation\n\n\nBicycle and Pedestrian Intern (Part-time) Mar 2015 – July 2015\n\nLearned ADA standards in order to digitize and assess statewide curb ramps and sidewalks in ArcMap\nLearned about bike/pedestrian safety and design and the benefits of complete streets through five day-long courses"
  },
  {
    "objectID": "cv.html#awards",
    "href": "cv.html#awards",
    "title": "Curriculum Vitae",
    "section": " Awards",
    "text": "Awards\n\n2022 Data Science Olympian: Collaborated with a team to compete in a Kaggle Challenge with Women in Big Data (WiBD)\nWisDOT Crash Dashboard won runner-up under Interactive Map category: 2021 Wisconsin Land Information Association map contest. View the project."
  },
  {
    "objectID": "cv.html#education",
    "href": "cv.html#education",
    "title": "Curriculum Vitae",
    "section": " Education",
    "text": "Education\n\n\n\n\n\nUniversity of Wisconsin - Madison\n\n\nMaster of Science 2012 - 2014\n\nUrban and Regional Planning\nCertificate in Transportation Management and Policy\n\n\n\n\n\n\nUniversity of Wisconsin - Oshkosh\n\n\nBachelors of Science 2006 - 2010\n\nGeography (GIS) and Urban Planning"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Jacci Ziebert",
    "section": "",
    "text": "I am a data analyst and R programmer. My passion for R sparked when I felt frustrated of the mundane workflow at my previous job from SAS/Excel and knew there had to be a better way. I took an online programming class and immediately applied what I learned to my job.\nSince then, I’ve built an R package to improve work flow, developed Shiny web apps, and automated long reports. I enjoy translating data into a concise and meaningful visual for a general audience in either a dashboard or a report.\nWhen I’m not coding, I enjoy graphing my coffee roasts, playing strategy board games, and cycling.\n\n\n\nGetting ready for a morning gravel ride"
  },
  {
    "objectID": "about.html#greetings",
    "href": "about.html#greetings",
    "title": "Jacci Ziebert",
    "section": "",
    "text": "I am a data analyst and R programmer. My passion for R sparked when I felt frustrated of the mundane workflow at my previous job from SAS/Excel and knew there had to be a better way. I took an online programming class and immediately applied what I learned to my job.\nSince then, I’ve built an R package to improve work flow, developed Shiny web apps, and automated long reports. I enjoy translating data into a concise and meaningful visual for a general audience in either a dashboard or a report.\nWhen I’m not coding, I enjoy graphing my coffee roasts, playing strategy board games, and cycling.\n\n\n\nGetting ready for a morning gravel ride"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Jacci Ziebert",
    "section": "",
    "text": "I am a data analyst and R programmer. My passion for R sparked when I felt frustrated of the mundane workflow at my previous job from SAS/Excel and knew there had to be a better way. I took an online programming class and immediately applied what I learned to my job.\nSince then, I’ve built an R package to improve work flow, developed Shiny web apps, and automated long reports. I enjoy translating data into a concise and meaningful visual for a general audience in either a dashboard or a report.\nWhen I’m not coding, I enjoy graphing my coffee roasts, playing strategy board games, and cycling.\n\n\n\nGetting ready for a morning gravel ride"
  },
  {
    "objectID": "index.html#greetings",
    "href": "index.html#greetings",
    "title": "Jacci Ziebert",
    "section": "",
    "text": "I am a data analyst and R programmer. My passion for R sparked when I felt frustrated of the mundane workflow at my previous job from SAS/Excel and knew there had to be a better way. I took an online programming class and immediately applied what I learned to my job.\nSince then, I’ve built an R package to improve work flow, developed Shiny web apps, and automated long reports. I enjoy translating data into a concise and meaningful visual for a general audience in either a dashboard or a report.\nWhen I’m not coding, I enjoy graphing my coffee roasts, playing strategy board games, and cycling.\n\n\n\nGetting ready for a morning gravel ride"
  },
  {
    "objectID": "noncode_projects/lost_city.html",
    "href": "noncode_projects/lost_city.html",
    "title": "Madison’s ‘Lost City’ - A Look Into the Failed 1920’s Lake Forest Development",
    "section": "",
    "text": "This article was originally published on Reddit in 2016 and was even featured in The Capital Times.\n\nA map geek, a planning enthusiast and a sucker for the unknown, I decided to delve a little deeper into one of Madison’s greatest mysteries, the “Lost City.” You may be thinking, “Is it even real?” or “Why the heck are there paved paths in the Arboretum?” After scoping out online resources, a visit to the Wisconsin Historical Society, and an in-situ visit to the area, I want to share what I found.  Let’s start with some context of planning. The industrial revolution brought an influx of people to flock to urban areas. By the late 1910s, it was the first time in human history that the country’s urban population outnumbered the rural population. This brought on the perception that cities were rifled with poor sanitation, terrible pollution, and overcrowding. Areas that were once used as public space or parks were now houses, businesses, and industries. People wanted to escape dirty, crowded cities, but did not want to live in farm country.\n\nStarting in the 1890s and lasting til the 1920s, a new wave of planning cities emerged deemed the City Beautiful Movement. Led by developers, planners, and architects, the idea was that classic beauty and monumental grandeur of the city would inspire feelings of civic loyalty and moral rectitude. By incorporating civic centers, parks, and grand boulevards, city beautification would reduce crime, reduce density, and increase green space. While Madison was already established as being a pristine place to live, there were worries with what the city’s predicted continued growth will lead.\n\nDeveloper and president of the Lake Forest Company, Chandler B. Chapman attempted to hop on the bandwagon in taking one step further in beautifying Madison by building a residential paradise called Lake Forest. An 800-acre epic development with over 1,000 lots, Lake Forest would be an escape from city life with numerous parks, its own public transit, and a city center. The company promoted Lake Forest by calling it “by far the best planned city residential section in Wisconsin” and “the most beautiful, modern, healthful, and desirable dwelling spot in the Northeast.”\n\nThis southeastern section of the development was started with many of the roads built along with a few houses.\n Note: Block A is the Civic Center and Block B is St. Cyr Circle while Capitol Avenue intersects these two circles.\n\n\n\n\n\n\n\n\n\nThe center of life for Lake Forest residents, the Civic Center was planned to have small shops and businesses lined around the circle. Separating land uses, such as commercial, industrial, and residential, was a new phenomenon in planning. The only building still standing today is the pumping station which, I believe, is still in use.\n\nA network of lagoons were proposed for two prominent reasons: to provide waterfront access to residents and to stabilize the marshy grounds.\n\nThis was one grandeur development! To entice people in purchasing property, the company heavily advertised. Above is a 1920 advertisement placed in the Wisconsin State Journal. Additionally, a bimonthly newsletter called the “Lake Forester” that ran for 1.5 years from 1921-1922 was written to chronicle the progress of the development.\n\nIn 1921, the Lake Forest Company broke grounds: roads were pored with concrete, dredging was being done and housing foundations were laid. This is Capital Avenue under construction. Planned to be 120 feet wide with sidewalks on each side with an unforgettable view of the Capital Building, the size would have been comparable to Washington Avenue today. Street cars were planned to run in the center, providing transportation to the downtown. You can see St. Mary’s Hospital to the slight left.\n\nWhile at least five houses (some sources say 7 houses) were completed, none of them were ever occupied. EDIT: 10/8/2016 An Arboretum researcher says some of the houses are actually still standing and occupied.\n\nWhat happened next, nobody foresaw. A combination of problems led to the collapse and eventual decay of the development. Dredging proved insufficient: roads and foundations were sinking due to marshy soils. In 1920, only 61 lots were sold while the company’s financial planner went bankrupt. The Great Depression and World War II didn’t help either. Strapped for cash, the Lake Forest Company sold some of the land to the University of Wisconsin for arboretum purposes in the 1930s. Ultimately the development failed. This area today is the Lost City Forest in the Arboretum, perhaps a different kind of paradise.\n\nYup, there’s actually Indian effigy mounds here!  This 1941 plat map shows two effigy mounds located north of Lake Forest. Furthermore, a hand-drawn 1915 map of effigy mounds I found on Arboretum’s online map shows a group of four mounds here, one looking like a panther. Today this is now a residential area around Marshall Parkway. It’s private property and I wonder if the mounds still exist.  The mound site at St. Cyr Circle the article mentions, I found no records of a panther mound here. The 1915 map shows two long narrow mounds at this location while another map indicates that no mounds exist. However, there is a large group of at least 8 mounds located inside Wingra Woods, just west of Lake Forest.\n\nA 1937 aerial photo of the area shows the remains of this epic development. As you can seen, only some of the roads were completed. Today the roads east of the former Civic Center is completely developed with houses and apartments, while the other roads remain crumbling into history.\n\nToday, not much remains. A walk into Lost City Forest you find mysteriously paved paths nearly a century old. If you look at the Imagery basemap at ArcGIS.com you could actually see outlines of these roads.\n My findings cannot be complete without a stroll through the woods!\n\n\n\n\n\n\n\n\n\nI bet some of you are tempted to look for the remains. The Arboretum hosts a once-a-year Lost City tour, typically at the end of October. I went to the presentation and I highly recommend it.  The Arboretum advises hikers to not wonder off the trails as to protect the environment. I don’t want to encourage nor discourage people from doing so, just wanted to share a piece of this fascinating mystery.\nIf you’re curious, a few adventurers sought to find the Lost City. Check ’em out here:\n\nhttps://badgerherald.com/artsetc/2015/09/14/madisons-lost-city-inside-the-forgotten-remains-of-lake-forest/\nhttp://never365.blogspot.com/2013/11/day-177-finding-lost-city.html\nhttps://www.youtube.com/watch?v=GpYXDLR4_f0&feature=youtu.be\n\nHope you enjoyed this!\nSources:\n\nArcGIS Online\nLake Forester, 1920 - 1921 found at University of Wisconsin Collection\nMadison:the illustrated sesquicentennial history Vol. 1 1856 - 1931\nWisconsin Historical Aerial Image Finder\nWisconsin Historical Society"
  },
  {
    "objectID": "projects/coffee.html",
    "href": "projects/coffee.html",
    "title": "An Analysis of James Hoffman’s ‘The Great American Coffee Taste Test Live Stream’",
    "section": "",
    "text": "The “Great America Taste Test” is what coffee expert and British YouTuber James Hoffman called the live stream coffee tasting event held in October 2023. 5,000 Americans participated in this simultaneous coffee sipping event where each participant tasted the same four coffees. They were then asked to fill out a survey of around 100 questions ranging from coffee drinking habits to describing the coffee flavors of the tested coffees. Of the 5,000 participants, 4,042 surveys were completed.\nBeing a coffee nerd and a data analyst, I decided to analyze the results! James did do a follow-up video on his own analysis on the survey so I don’t want to overlap too much on his findings.\nI’ll go through the participant demographics, spending habits, coffee preferences, and coffee tasting. I looked at how bitterness and acidity was rated in each coffee then I went deeper to find if there were commonalities in coffee flavor notes via a text analysis."
  },
  {
    "objectID": "projects/coffee.html#so-what-were-the-four-coffees",
    "href": "projects/coffee.html#so-what-were-the-four-coffees",
    "title": "An Analysis of James Hoffman’s ‘The Great American Coffee Taste Test Live Stream’",
    "section": "So What Were The Four Coffees?",
    "text": "So What Were The Four Coffees?\nThe four different coffees, Table 1, were each of a different roast: light, medium, and dark. The fourth coffee, Coffee D, was a light roast coffee but with a unique processing method. The method was anaerobic natural fermentation, meaning that coffee cherries were kept in a closed container with no oxygen as to allow for fermentation of the raw coffee beans - this tends to give the coffee a heavy fruity and fermented taste. Yum!\n\n\n\n\nTable 1: The Four Coffees Tasted\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCoffee\nRoast Level\nOrigin\n\n\n\n\n\n\nCoffee A\n\nLight\nSingle - Kenya\n\n\n\n\nCoffee B\n\nMedium\nMultiple - Blend\n\n\n\n\nCoffee C\n\nDark\nMultiple - Blend\n\n\n\n\nCoffee D\n\nLight\nSingle - Columbia\n\n\n\n\n\n\n\n\n\n\nBased on these four coffees, how do you think the tastes will compare? Which would you think are more bitter or more acidic? Do you think you would have a favorite?"
  },
  {
    "objectID": "projects/coffee.html#demographics",
    "href": "projects/coffee.html#demographics",
    "title": "An Analysis of James Hoffman’s ‘The Great American Coffee Taste Test Live Stream’",
    "section": "Demographics",
    "text": "Demographics\nNearly 3 out of 4 (74%) participants were between ages 25 and 44 years (Table 2) which is much higher compared to the worldwide YouTube audience where only 37% are between these ages 1. Participants are also overwhelming male representing 62% of participants verse U.S. YouTube demographics where 49% are male 2. We can say James’ audience tend to be younger and male, even accounting for YouTube bias.\n\n\nI also looked at the breakdown of daily coffee drinking by age in Table 2. Interestingly, we see a trend in drinking more coffee per day as one ages. Who said college students drink lots of coffee, maybe we should say it’s the retirees who drink far more because of their greater spare time?\n\n\n\n\n\n\n\n\n\nFigure 1: Participants by Gender\n\n\n\n\n\n\n\n\n\n\n\nTable 2: Participant Age and Daily Coffee Drinking Habits\n\n\n\n\n\n\n\n\n\nAge Group\nn\nAge Group Prop\n\n0-1 cups||2-3 cups||4 or More\n\n\n\n\n\n&lt;18 years old\n19\n\n\n0%\n\n\n\n\n\n   58%42%0%\n\n\n\n18-24 years old\n447\n\n\n11%\n\n\n\n\n\n   53%43%4%\n\n\n\n25-34 years old\n1956\n\n\n50%\n\n\n\n\n\n   44%53%3%\n\n\n\n35-44 years old\n948\n\n\n24%\n\n\n\n\n\n   36%59%5%\n\n\n\n45-54 years old\n296\n\n\n8%\n\n\n\n\n\n   34%57%9%\n\n\n\n55-64 years old\n182\n\n\n5%\n\n\n\n\n\n   25%64%12%\n\n\n\n&gt;65 years old\n94\n\n\n2%\n\n\n\n\n\n   28%57%15%\n\n\n\n\n100 participants were removed due to null response values."
  },
  {
    "objectID": "projects/coffee.html#coffee-expertise",
    "href": "projects/coffee.html#coffee-expertise",
    "title": "An Analysis of James Hoffman’s ‘The Great American Coffee Taste Test Live Stream’",
    "section": "Coffee Expertise",
    "text": "Coffee Expertise\nParticipants were asked to rate one’s own coffee expertise on a scale of 1 - 10. Not surprisingly, we see participants lean towards the upper half with nearly half (45%) rating themselves as a 6 or 7. However, there is still a significant number on the lower end and very few at a 9 or 10, pulling the overall average down to 5.7. As to more easily compare these groups throughout my analysis, I binned one’s level of expertise into five groups. Now, the ‘5-6’ and ‘7-8’ expertise groups make up about the same proportion (36% each) and the ‘9-10’ expertise group make up only 3% of total participants.\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Self-Rated Coffee Expertise\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: Binned Self-Rated Coffee Expertise Groups"
  },
  {
    "objectID": "projects/coffee.html#coffee-habits",
    "href": "projects/coffee.html#coffee-habits",
    "title": "An Analysis of James Hoffman’s ‘The Great American Coffee Taste Test Live Stream’",
    "section": "Coffee Habits",
    "text": "Coffee Habits\n\nCoffee Consumption\n\n\nLet’s explore the coffee drinking habits by expertise group. Figure 5 shows a trend in increasing coffee consumption as one learns more about coffee. Some even drinking 4 or more cups per day with nearly 1 out of 10 participants for the ‘9-10’ expertise group. Figure 4 shows a similar correlation in which participants tend drink coffee just black - meaning no milk, sugar, or other additive (like olive oil, gross) - as one gets more knowledgeable.\n\n\n\n\n\n\n\n\n\nFigure 4: If Participants Drink Coffee ‘Just Black’ by Expertise Group\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 5: Daily Coffee Drinks by Expertise Group\n\n\n\n\n\nFigure 6 shows that a pour over is up to 8x more likely to be consumed by those with more coffee expertise compared to those with little expertise. Perhaps this explains why so many prefer to drink coffee black - pour overs tend to use light roast beans as they tend to retain more of their origin flavor and unique elements. A pour over is my daily driver to brew coffee every morning. Interestingly, participants across all expertise groups consistently drink regular drip coffee- I suppose that method is still very quick and convenient whether drinking at home, buying from a gas station, or brewing at work. Admittedly, I’m still confused how an ‘expert’ coffee drinker says their favorite coffee comes from a drip coffee as this method allows for the least control over variables that make great coffee.\n\n\n\n\n\n\n\n\nFigure 6: Favorite Coffee Drink of Participants\n\n\n\n\n\n\n\nSpending Habits\nAs one would expect, as one becomes more experienced in a hobby, one spends more money. That was very true for me as I fell into black hole of striving to perfect my coffee game, wanting to buy the top gear given my budget.\nFor the folks with less expertise, nearly half (48%) of them spend less than $20 a month. Figure 5 shows that this group typically drinks far less coffee with most (74%) drinking 0-1 cups per day. Given their low spending, they probably don’t drink a lot of specialty coffee since a bag of beans costs $10-20 nor drink coffee at specialty cafes too often.\nWe find similar a pattern in low spending when asked how much a participant spent on coffee equipment over the past five years. While James says having a good coffee grinder can greatly improve a cup of coffee, a quality burr grinder typically costs more than $100. This implies these folks probably make terrible coffee with a drip coffee maker, but who am I to judge?\nFor the upper end - the ‘9-10’ expertise group, most (94%) spend at least $20 a month and over half (58%) have spent over $1,000 over the past 5 years on coffee equipment. Given that this group is much more likely to drink pour over - a method where one may own a goose neck electric kettle, coffee scale, Chemex, and a decent grinder which can easily add up to a few hundred dollars. And they are also more likely to drink espresso (Figure 6) in which they may own an espresso machine at home, a machine that can easily cost over $1,000.\n\n\n\n\n\n\n\n\nFigure 7: Money Spent Each Month on Coffee by Coffee Expertise\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 8: Money Spent Over the Past 5 Years on Coffee Equipment by Coffee Expertise"
  },
  {
    "objectID": "projects/coffee.html#coffee-cupping",
    "href": "projects/coffee.html#coffee-cupping",
    "title": "An Analysis of James Hoffman’s ‘The Great American Coffee Taste Test Live Stream’",
    "section": "Coffee Cupping",
    "text": "Coffee Cupping\n\nWhat is Coffee Cupping?\nMost of us have probably seen tasting notes when purchasing coffee (i.e. grapefruit, chocolate) and the roast level but are these descriptors actually helpful? Would someone describe similar notes of the same coffee?\n Participants tasted the coffee via coffee cupping, a coffee tasting technique where a taster takes and spoonful of coffee and slurps it as to asses aspects such as cleanness, sweetness, acidity, mouthfeel and aftertaste as in the above picture. But why the slurp? Slurping aerates the coffee as it makes contact with taste buds which intensifies the tasting sensation.\nTo answer these questions, I looked at how participants ranked the level of bitterness and acidity of each coffee. Then they were asked to write down tasting notes so I will explore this via a text analysis to identify the top flavor notes used to describe each coffee.\n\n\nBitterness and Acidity\nParticipants were asked to rate the level of bitterness and acidity of each coffee on a scale of 1-5. Table 3 and Table 4 shows the average and the distribution of this 5-point scale. We see (Coffee A and Coffee D) and (Coffee B and Coffee C) showed a similar average and similar distribution for both bitterness and acidity.\nTo find if these differences were statistically different these two pairs of coffee, I performed a one-sided t-test with a 95% confidence level.\nFor bitterness, I found that Coffee C was statically more likely to be bitter than Coffee B (p=0.005) and Coffee D was not more bitter than Coffee A (p=0.178). We can statistically say the order of bitterness:\nCoffee C &gt; Coffee B&gt; Coffee A/Coffee D\nFor acidity, I found that Coffee D was statically more likely to be more acidic than Coffee A (p=0) and Coffee C was statically more likely to be more acidic than Coffee B (p=0). Again, we can statistically say the order of acidity:\nCoffee D &gt; Coffee A &gt; Coffee C &gt; Coffee B\nI did perform a t-test for all other combos of coffees and found them all to be statisically different.\n\n\n\n\n\n\nTable 3: Bitterness Statistics of Each Coffee\n\n\n\n\n  \n  \n\n\n\n\n\n\n\n\n\nCoffee\nAverage Bitter\nStandard Deviation\nDistribution of Rank (1-5)\n\n\n\n\n\nCoffee A\n\n2.14\n0.95\n\n    \n\n\n\n\nCoffee B\n\n3.01\n0.99\n\n    \n\n\n\n\nCoffee C\n\n3.07\n1.00\n\n    \n\n\n\n\nCoffee D\n\n2.16\n1.08\n\n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 4: Acidity Statistics of Each Coffee\n\n\n\n\n  \n  \n\n\n\n\n\n\n\n\n\nCoffee\nAverage Acidity\nStandard Deviation\nDistribution of Rank (1-5)\n\n\n\n\n\nCoffee A\n\n3.63\n0.98\n\n    \n\n\n\n\nCoffee B\n\n2.22\n0.87\n\n    \n\n\n\n\nCoffee C\n\n2.37\n0.92\n\n    \n\n\n\n\nCoffee D\n\n3.86\n1.01\n\n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 5\n\n\n\n\n\n\n\n\nTaste Notes\nCool we found a correlation in bitterness and acidity in the coffees! Let’s dig deeper into what the coffee tasted like, as described by the participants. So What does the coffee remind you of? was a repeated question James asked to describe the flavor notes during the live stream. Coffee tastes like coffee, but there are nuances in the flavor notes that have a similar mouthfeel as other foods.\nComparing multiple coffees side-by-side allows one to discover the nuances of each coffee. One may start with a general words such as nutty but then move on to identify which type of nut. Below is a coffee flavor wheel3, a tool to help identify words for one’s taste buds (this wheel was NOT used during the live event).\n\n\n\n\n\n\n\n\nFigure 9: Coffee Flavor Wheel - hover to read descriptions\n\n\n\n\nParticipants were asked to describe the coffee flavors in an open text field. After some cleaning of these words, I performed a text analysis to identify and count key flavor words, such as bright or grapefruit. I took the top 25 most frequent words of each coffee and created a word cloud. The larger the word, the frequent the word was mentioned (the count of each word is in the hover text).\n\n\n\nCoffee A: Top Word was Fruity with 616 Mentions\n\n\n\n\n\n\nCoffee B: Top Word was Chocolate with 600 Mentions\n\n\n\n\n\n\n\n\nCoffee C: Top Word was Chocolate with 332 Mentions\n\n\n\n\n\n\nCoffee D: Top Word was Fruity with 1,085 Mentions\n\n\n\n\n\nFor me, Coffee A conjures up images of something light, bright and fruity. While participants described Coffee B and Coffee C as chocolate, nutty, and balanced and even a few described them as burnt. Coffee C had more of an even spread in word counts as words are similar in size. Coffee D, however, paints a much different image. Fermented and funky made it to the top - given this was a fermented natural processed coffee, this is not surprising!\nOkay - so let’s see how these top 25 tasting notes match up to the flavor wheel. ‘Acid’, ‘Bitter’, ‘Sweet’, ‘Fruity’, and ‘Chocolate’ are generic words used to describe all 4 coffees, though at different frequencies as demonstrated in the word clouds. We do see a pattern for Coffee A and Coffee D, in yellow, compared to Coffee B and Coffee C, in brown, in that the former is described as much more fruity and floral as opposed to roasted and smoky. Coffee D was described by many specific types of berries.\n\n\n\n\n\n\n\n\nFigure 10: The Colors Represent Which Taste Notes Overlap Which Coffee (Hover to see the overlap)\n\n\n\n\n\n\nCoffee Preferences\n\n\n\n\n\n\n\n\nFigure 11: Favorite Coffee by Experience Group\n\n\n\n\n\nSo going back to expertise groups, let’s see if there’s a correlation in preference of coffee. Participants were asked to describe their coffee preference before the day’s tasting: Table 6 below is a heatmap of breakdowns of these preferences by expertise group. Figure 11 above is a breakdown of favorite coffee by expertise group.\nWe see those with expertise ‘1-2’ and ‘3-4’ have a preference for chocolatey coffee. As we’ve previously seen in the word clouds, chocolate was the number one word mentioned for Coffee B and Coffee C and a latte was the top coffee for many of these folks (Figure 6), a drink that typically uses a medium or dark roast coffee. So it makes sense to see their favorite coffee was either Coffee B or Coffee C.\nAs for the ‘7-8’ and ‘9-10’ expertise groups, many have a preference for fruity and juicy coffee. Around 40% of these groups have pour over (Figure 6) as their preferred drink, a drink that typically uses a lighter roast. It makes sense their favorite drink is a light roast. Though it’s interesting that many prefer the more exotic of the two with nearly half had a preference for Coffee D.\n\n\n\n\nTable 6: Heatmap of Coffee Preferences Before Tasting by Expertise Group\n\n\n\n\n\n\n\n\n\nFlavor\n1-2\n3-4\n5-6\n7-8\n9-10\n\n\n\n\nBold\n8%\n6%\n6%\n3%\n2%\n\n\nBright\n4%\n7%\n10%\n10%\n9%\n\n\nCaramalized\n10%\n9%\n8%\n5%\n6%\n\n\nChocolatey\n24%\n22%\n16%\n13%\n5%\n\n\nFloral\n3%\n3%\n5%\n6%\n13%\n\n\nFruity\n7%\n12%\n23%\n33%\n36%\n\n\nFull Bodied\n12%\n18%\n14%\n8%\n7%\n\n\nJuicy\n1%\n3%\n6%\n10%\n14%\n\n\nNutty\n9%\n10%\n8%\n6%\n2%\n\n\nSweet\n21%\n9%\n6%\n5%\n6%"
  },
  {
    "objectID": "projects/coffee.html#conclusion",
    "href": "projects/coffee.html#conclusion",
    "title": "An Analysis of James Hoffman’s ‘The Great American Coffee Taste Test Live Stream’",
    "section": "Conclusion",
    "text": "Conclusion\nThis cupping live stream demonstrates there’s actually some commonalities between tasting by the cupping participants. Coffee A and Coffee D were both fruity, while Coffee D was described as ‘fermented’. Coffee B and Coffee C also had similar flavor notes, but Coffee C was statistically described as being more bitter, on average. This analysis also demonstrates there’s not much flavor differences in medium and dark roasts as compared to light roasts given that similar words were used to describe Coffee B and Coffee C.\nNow time to make a pour over!"
  },
  {
    "objectID": "projects/coffee.html#footnotes",
    "href": "projects/coffee.html#footnotes",
    "title": "An Analysis of James Hoffman’s ‘The Great American Coffee Taste Test Live Stream’",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhttps://datareportal.com/essential-youtube-stats↩︎\nhttps://www.statista.com/statistics/810461/us-youtube-reach-gender/↩︎\nSource: World Coffee Research - Sensory Lexicon https://worldcoffeeresearch.org/resources/sensory-lexicon↩︎"
  },
  {
    "objectID": "projects/crash_comparison.html",
    "href": "projects/crash_comparison.html",
    "title": "WisDOT Crash Comparison Dashboard",
    "section": "",
    "text": "View the Dashboard\n\nWhat I learned\nKnowing this dashboard would be viewed on a variety of screen sizes, I used dynamic font sizes (i.e. 1.2em) as opposed to static font sizes (i.e. 12pt). I also wrote CSS as to make printing of the entire dashboard possible on two pages.\n\n\nHighlighted packages\nThe dashboard uses dashboardthemes, ggplot2 and ggtext using the golem framework."
  },
  {
    "objectID": "projects/crash_facts.html",
    "href": "projects/crash_facts.html",
    "title": "Wisconsin Traffic Crash Facts",
    "section": "",
    "text": "2021 Wisconsin Crash Facts\n\nWhat I learned\nThis project gave me an excuse to learn Quarto! I spent most time writing functions to standardize the table format, to render ggplot charts, and to aggregate the data. Then it was just a matter of creating each chapter using these functions.\n\n\nHighlighted packages\ngt, gtExtras, quarto"
  },
  {
    "objectID": "projects/json_api.html",
    "href": "projects/json_api.html",
    "title": "Working with API and JSON format",
    "section": "",
    "text": "What I learned\nThis was done it two parts 1) Writing functions that pulls data from our court case API into a JSON format over a certain time period and for certain citations. This data was flattened and compiled into a single dataframe and exported. And 2) Finding the sentence length for a certain citation for each case. This was tricky as one case may have multiple citations while sentence lengths can be found in multiple branches of the flattened JSON. I solved this issue by finding which branch a certain location was found and replaced part of the branch name with where the sentence length location. I was able to extract the sentence length with this ‘renaming.’ to calculate the average sentence length.\n\n\nHighlighted packages\njsonlite, httr"
  },
  {
    "objectID": "projects/roasting_dashboard.html",
    "href": "projects/roasting_dashboard.html",
    "title": "Coffee Roasting Profiler",
    "section": "",
    "text": "Dashboard  Code\n\nWhat I learned\nWhile I use Artisan, an open-source software used to record coffee roasts, I wanted an interactive chart to explore my roasts.\nThe tricky part was adding the RoR (rate of return) curves (the blue and purple ines) onto the graph as this is calculated via an algorithm written in Python inside Artisan. I found this piece of code and rewrote part of it so it would work for my app. I used the reticulate package so the app can read Python code.\n\n\nHighlighted packages\nThe dashboard uses reticulate, formattable, plotly, and DT under a golem framework."
  }
]